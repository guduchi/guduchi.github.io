<html>
  <head><meta name="generator" content="Hexo 3.8.0">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
  
    <link rel="icon" href="/images/favicon.ico">
  
  <title>yunqiu.blog</title>
  <link rel="stylesheet" href="/css/styles.css">
  <script src="/lib/jquery.js"></script>
  <link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css">
  <script src="/js/common.js"></script>
  <script src="/lib/bootstrap/js/bootstrap.js"></script>
  <link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
</head>

  <body>
    <header class="header">
  <div class="color-bar">
    <ul>
      <li class="bar1"></li>
      <li class="bar2"></li>
      <li class="bar3"></li>
      <li class="bar4"></li>
      <li class="bar5"></li>
      <li class="bar6"></li>
    </ul>
  </div>
  <div class="header-wrapper">
    <div class="blog-title">
      
        <img class="logo" src="/images/logo.png" alt="logo">
      
      <a href="/" class="title">yunqiu.blog</a>
    </div>
    <nav class="navbar navbar-menu">
      <div class="container-fluid">
        <div class="navbar-header">
          <button type="button" id="nav-btn" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
        </div>
      </div>
        <ul class="menu menu-pc">
          
            <li class="menu-item">
              <a href="/" class="menu-item-link"><i class="fa fa-home"></i><span>首页</span></a>
            </li>
          
            <li class="menu-item">
              <a href="/tags/" class="menu-item-link"><i class="fa fa-tags"></i><span>标签</span></a>
            </li>
          
            <li class="menu-item">
              <a href="/archives/" class="menu-item-link"><i class="fa fa-file"></i><span>归档</span></a>
            </li>
          
            <li class="menu-item">
              <a href="https://github.com/guduchi" class="menu-item-link"><i class="fa fa-github"></i><span>github</span></a>
            </li>
          
        </ul>
    </nav>
  </div>
</header>
<div class="collapse nav-collapse" style="position:fixed;top:59px;" id="bs-example-navbar-collapse-1">
  <ul class="menu menu-mobile" style="list-style:none">
    
      <li class="menu-item">
        <a href="/" class="menu-item-link"><i class="fa fa-home"></i><span>首页</span></a>
      </li>
    
      <li class="menu-item">
        <a href="/tags/" class="menu-item-link"><i class="fa fa-tags"></i><span>标签</span></a>
      </li>
    
      <li class="menu-item">
        <a href="/archives/" class="menu-item-link"><i class="fa fa-file"></i><span>归档</span></a>
      </li>
    
      <li class="menu-item">
        <a href="https://github.com/guduchi" class="menu-item-link"><i class="fa fa-github"></i><span>github</span></a>
      </li>
    
  </ul>
</div>

    <main class="main">
      <div class="content">
        
  <article class="post">
    <div class="post-title">
      <h2 class="title">spark</h2>
       
    </div>
     <div class="post-meta">
      <span class="post-time">2019-05-27</span>
    </div>
    <div class="post-content">
      <p>概要</p>
<p>RDD是数据集并不存储数据，他是最基本的抽象，记录一系列的转化关系与位置</p>
<p>spark不结合分布式文件系统，用本地文件系统只要一个机器没有需要的同样文件就不行</p>
<p>分布式文件系统不会，尽量用分布式文件系统</p>
<p>连接spark-shell  是很慢需要建立连接申请资源，等一些列动作</p>
<p>start-all.sh  先启动配置文件，在启动master，再通过ssh协议启动worker,启动worler又要读配置文</p>
<p>件。分配的核数不要超过自己的物理存储，你的事几核及线程</p>
<p>spark2.2.0 值支出hadoop2.6及以上的版本</p>
<p>spark中间结果能存下来的放入内存中，省着还要HDFS,放不下的放入磁盘中。</p>
<p>spark只负责计算，并不存储数据</p>
<p>Spark是MapReduce的替代方案，而且兼容HDFS、Hive，可融入Hadoop的生态系统，以弥补</p>
<p>MapReduce的不足。</p>
<p>spark是内存加磁盘，DAG模型比较先进</p>
<p>MR也有环形缓冲区放入内存，编程模型不怎么先进。</p>
<p>离线一般有hadoop</p>
<p>spark有全家桶</p>
<p>用spark2.2.0用java8，java7已经移除了</p>
<p>高可用master需要手动启动</p>
<p>提交任务的客户端要与master保持通信</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/5aae6fb66c8148a2a72dbc8fd7602fbd/spark%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2.png" alt="img"></p>
<p>加的参数要在jar包之前，因为是要给submit</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/557f9d0e066f4909b5e60e99f97b44ed/clipboard.png" alt="img"></p>
<p>执行计算是Coarse…..这个进程，执行完就销毁，释放资源</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/391f39820a31406d8f2b16f7db06c00e/922c6d2b59f847a0bac7507a3cfe9a2d.jpg" alt="img"></p>
<p>笔记</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/c7a399eb7cba4b58a56b95cbe009c286/2f0872aebee942669758061b48f92933.jpg" alt="img"></p>
<p>spark任务执行过程简介</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/8cc3c2ea8a6a4eda854c7d7352a3976f/spark%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%E7%AE%80%E4%BB%8B.png" alt="img"></p>
<p>读取本地文件每个worker都应该有你要读的文件，文件中还要有数据否则报错</p>
<p>提交任务spark-shell  是需要准备资源，启动资源所以很慢,与客户单端建立连接</p>
<p>启动spark-shell    没有显示在界面？？？？？？？？？？？？？没连接进集群–master</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/42b826d533dd4c5e9e867fc9ef8c84aa/55121c54e5b540b3b2e1ffe5a8e75c6e.jpg" alt="img"></p>
<p>接上图</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/d14c7f4450554c0bb465772d58aa6e6c/e58e57ab63124e20aac37552322e0fc5.jpg" alt="img"></p>
<p>spark-core_2.11  2.11代表scla  版本</p>
<p>spark读写hdfs数据就是用hadoop 的客户端</p>
<p>hadoop有几个结果文件就是指定几个reducertask</p>
<p>RDD简介</p>
<p>RDD的简介(1)</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/ebee48c8e65546b08fb8f75ca4dd38ed/rdd%E7%9A%84%E7%AE%80%E4%BB%8B%281%29.png" alt="img"></p>
<p>spark中的重要角色</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/f4e8b892541c4b0c801363fc01d12ccd/spark%E4%B8%AD%E7%9A%84%E9%87%8D%E8%A6%81%E8%A7%92%E8%89%B2.png" alt="img"></p>
<p>RDD</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/5235267e0baf41228374aff7de155095/1.png" alt="img"></p>
<p>rdd笔记</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/2dd9e2475b064463b2d39c0fddf52302/abb72a5472b04da884fb4b2e77b08193.jpg" alt="img"></p>
<p>spark shell</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/bf638602582e44eb9a426095f2fc32dc/179f8c9d169c44dbb58f1b92642d62b1.jpg" alt="img"></p>
<p>概念</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/c6e7aed9badf47769fe7e457d86a352e/ff9efb6d1842466ab10ab9689e99ae14.jpg" alt="img"></p>
<p>Rdd属性</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/8ae67b2cbf42432c9acf71c2cee13019/0b57ee75d8da4d58aefc1d10f1ef7676.jpg" alt="img"></p>
<p>接上图</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/af360bd1deea41d79c5a18a5560b8c95/f40e8e5880a841f2a03356165d9b9ab1.jpg" alt="img"></p>
<p>创建rdd</p>
<p>读hdfs文件的时候，里面有几个文件，并且文件很小，并且你没有指定切片，那就可能是四个切</p>
<p>片。默认切片不读hdfs，切片数就跟核数cores有关了，几个核几个切片，几个切片生成几个文</p>
<p>件。一般一个task读一个切片</p>
<p>action执行完不在返回RDD</p>
<p>默认两个分区2，可以自己设置，一般与你要处理的目录里面的文件有关系</p>
<p>一个分区可以产生一个task</p>
<p>常用Transformation</p>
<p>Rdd算子</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/6b0fab756fa649d78766f9aa47fe7674/7a9617a1dbc049c39288d76e31f21eff.jpg" alt="img"></p>
<p>join结果：</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/498f5e0b422e43e4b0709f9a65991956/clipboard.png" alt="img"></p>
<p>aggregate</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/0a9fc59ded6148a08452515fcd9c34ac/6871d2f322f64554aba1977f7fe78a20.jpg" alt="img"></p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/e02562ead69a46a98257453af73a3ead/clipboard.png" alt="img"></p>
<p>aggregateByKey          是transmantion</p>
<p>这个100只是在局部时候使用，全局聚合不用与aggregate不同</p>
<p>aggregate是action</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/838df9081c1f47e78da1b4989ab18b70/c7522435c1a7436cb68868fca1c96d31.jpg" alt="img"></p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/6261e4b27f8a417b8640aeafa9b63c24/2df8ea03fe9349ce9aa5b67848d488a7.jpg" alt="img"></p>
<p>val rdd5 = sc.parallelize(List(List(“a b c”, “a b b”),List(“e f g”, “a f g”), List(“h i j”, “a a b”)))</p>
<p>List先转换为Rdd，里面装着(List(“a b c”, “a b b”)等，rdd5.flatmap()是操作第一个Rdd发送打每台</p>
<p>机器上，rdd5.flatmap(_.flatmap) 里面的flatMap 是操作里面的数据。</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/df0a9be8934e4b36b7a3bcf44c76f3eb/cb9b328dd8664a6f8afe65d5fb3855cc.jpg" alt="img"></p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/856a7f56a22b4f2baf072f08444a46aa/c6d4d0d311b24d87ae74c4d16a28af23.jpg" alt="img"></p>
<p>如果是左链接，就是左面的一定显示，但右面的不一定</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/bf5d205a610f40708dc7a607cbf5b4e2/30fead27eb4a4d9c94e129218fed3757.jpg" alt="img"></p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/6751a232d6ca4e47b7ab9daddf8d4ade/bec47ebc02054a0dbb5cb56b1ddbd9e3.jpg" alt="img"></p>
<p>groupByKey 不用参数，效率低，它并不是局部先聚合,而是把相同的发到同一台机器上在进行聚</p>
<p>合，耗费资源。</p>
<p>斜分组把rdd1中的相同的key的value先聚合，在吧把rdd2的相同的key的value也聚合</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/1fc95ff191f3493f91b9747f227986bf/65a3a4c0dee74f7596049af786403020.jpg" alt="img"></p>
<p>mapPartitionsWithIndex</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/b9fe1bc312914a95b7025d7858e4a18a/f90c6ee4cc334bb5bd53d437f3e1ca2e.jpg" alt="img"></p>
<p>代码：</p>
<p>mapPartitionsWithIndexval func = (index: Int, iter: Iterator[Int]) =&gt; {  iter.map(x =&gt; “[partID:” +  index + “, val: “ + x + “]”)}val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)rdd1.mapPartitionsWithIndex(func).collect————————————————————————————————————————————————————————————–aggregatedef func1(index: Int, iter: Iterator[(Int)]) : Iterator[String] = {  iter.map(x =&gt; “[partID:” +  index + “, val: “ + x + “]”)}val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)rdd1.mapPartitionsWithIndex(func1).collect</p>
<p>mapPartitionsWithIndex 一次拿出一个分区（分区中并没有数据，而是记录要读取哪些数据，真正</p>
<p>生</p>
<p>成的Task会读取多条数据），并且可以将分区的编号取出来</p>
<p>功能：取分区中对应的数据时，还可以将分区的编号取出来，这样就可以知道数据是属于哪个分</p>
<p>区的（哪个区分对应的Task的数据）</p>
<p>错误</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/dd2ade89907042c1a44a7ac14457a6fc/1783a46012a54c0a9d80421c8b9f07b8.jpg" alt="img"></p>
<p>同上</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/1b992d618cb0468a98fe0b64f362e0ae/clipboard.png" alt="img"></p>
<p>同上</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/b7826e2cda9b4238894f941a16d300a2/clipboard.png" alt="img"></p>
<p>同上</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/d5669dba23d74a689815a8299171ce81/955a2666296f499d95e919113fa0e270.jpg" alt="img"></p>
<p>解决上面必须设置yarn提交到集群否则默认本地报上面几个错误</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/29e47414c4de496c986e7a98286ba812/c58fec101da24c2a8978c3d2057e6486.jpg" alt="img"></p>
<p>问题</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/7eab4d8420474293bb1b339f100c476a/3cd6fb9bd8b44a50afe0c7f630f5a495.jpg" alt="img"></p>
<p>解决</p>
<p>yarn.site.xml 添加</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/280b570a78284b43a46c8f9b9708a8ad/21c10d8abb0c428aa2bfe3052fa3c795.jpg" alt="img"></p>
<p>错误：一直卡在这，还有spark中报yarn什么的就是没配的原因</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/df5d828e22b842d298e0c3e2c925e4c0/6a19c371bb514dbba1cfc36eabe4f6f0.jpg" alt="img"></p>
<p>解决把export HADOOP_CONF_DIR=/opt/hadoop-2.7.3/etc/hadoop  hadoop的配置文件配到</p>
<p>spark-env.sh就好啦。</p>
<p>未解决：？？？？？？？？？？？？？？？？？？？？？ 计算时候</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/89d24ebab19b45119d6c49039a5f722a/92ffd7b2b95649eb840562308601816f.jpg" alt="img"></p>
<p>注意：</p>
<p>内存配置，yarn配置，看日志是否报错，有的时候起来正常但是在报错，所以可能计算不了</p>
<p>未解决？？？？？？？？？？？？？？？？？？？提交计算</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/d0d05fcbaa5842c4b43e166060dcfc47/6975fb38cfda4bdeaa82acb377c2a863.jpg" alt="img"></p>
<p>错误</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/42596af3659449859f3fb7cfa1159a06/7b00d0b4f6644da6a79051250ddb4b93.jpg" alt="img"></p>
<p>mapPartitionsWithIndex 带有分区标号，除了记录数据属于哪个分区，每个分区有标号</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/6c74915b57154f68844505074b7b6448/d21595b2959142069162ec3b2a826324.jpg" alt="img"></p>
<p>aggregate 现在每个分区局部聚合，在把全局分区的所有在聚合，全局聚合</p>
<p>取每个分区最大的，在多个分区聚合（0）初始值</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/8d7da6dbd8224c16bec18f9f2e3202f9/b1beaf9453274fba918e2d19c2a18fdb.jpg" alt="img"></p>
<p>val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)</p>
<p>第一个分区 1,2,3,4   第二个分区  5,6,7,8,9？？？？？？？？</p>
<p>习题 19</p>
<p>初始值5应用到局部在全局聚合页应用，5 + 9 + 5</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/36d478362b374ac4bea8e779c9ea32d2/ad3834a603b04369b2a319f473ce4d16.jpg" alt="img"></p>
<p>  2 + 4 + “”</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/c5581a7d3ecb44e2bef19c2be0ea48e8/f88c58a7a46a4601ac7686a1be3cc4dd.jpg" alt="img"></p>
<p>“”  与 12 比变成 0 最小在转为字符串“0” 长度为1   下个分区为 0 ，转字符串为 0 长度为0</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/41da8056ea054ea5ad511721210a39b5/59cb14d9520a47a696e6c146d6668551.jpg" alt="img"></p>
<p>1 + 1</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/27e39b6bfe9a44c498cbb7e9a391a3c3/1e19d58d06904861a8dbdbc8b9ba5824.jpg" alt="img"></p>
<p>aggregateByKey  key相同的先局部在全局，但是初始值只在局部有用</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/4fd23f37852e41deab356fa6ec74b4e8/6787c1fbe80d430cb372f76d490319b3.jpg" alt="img"></p>
<p>代码：</p>
<p>aggregatedef func1(index: Int, iter: Iterator[(Int)]) : Iterator[String] = {  iter.map(x =&gt; “[partID:” +  index + “, val: “ + x + “]”)}val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)rdd1.mapPartitionsWithIndex(func1).collect先分区内局部聚合在进行全局聚合 aggregaterdd1.aggregate(0)(math.max(_, _), _ + <em>) 现在每个分区取出最大值，在将每一个分区的结果进行叠加，全局叠加也会应用初始值rdd1.aggregate(5)(math.max(</em>, _), _ + <em>)val rdd2 = sc.parallelize(List(“a”,”b”,”c”,”d”,”e”,”f”),2)def func2(index: Int, iter: Iterator[(String)]) : Iterator[String] = {  iter.toList.map(x =&gt; “[partID:” +  index + “, val: “ + x + “]”).iterator}先每个分区在，全局所有分区结果聚合rdd2.aggregate(“”)(</em> + _, _ + <em>)rdd2.aggregate(“=”)(</em> + _, _ + <em>)val rdd3 = sc.parallelize(List(“12”,”23”,”345”,”4567”),2)rdd3.aggregate(“”)((x,y) =&gt; math.max(x.length, y.length).toString, (x,y) =&gt; x + y)val rdd4 = sc.parallelize(List(“12”,”23”,”345”,””),2)rdd4.aggregate(“”)((x,y) =&gt; math.min(x.length, y.length).toString, (x,y) =&gt; x + y)val rdd5 = sc.parallelize(List(“12”,”23”,””,”345”),2)rdd5.aggregate(“”)((x,y) =&gt; math.min(x.length, y.length).toString, (x,y) =&gt; x + y)————————————————————————————————————————————————————————————–aggregateByKeyval pairRDD = sc.parallelize(List( (“cat”,2), (“cat”, 5), (“mouse”, 4),(“cat”, 12), (“dog”, 12), (“mouse”, 2)), 2)def func2(index: Int, iter: Iterator[(String, Int)]) : Iterator[String] = {  iter.map(x =&gt; “[partID:” +  index + “, val: “ + x + “]”)}pairRDD.mapPartitionsWithIndex(func2).collectpairRDD.aggregateByKey(0)(math.max(</em>, _), _ + <em>).collect这个100只是在局部时候使用，全局聚合不用与aggregate不同pairRDD.aggregateByKey(100)(math.max(</em>, _), _ + _).collect</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/2c6f1e67733c410bb12ef296f6e07783/aggregatebykey.png" alt="img"></p>
<p>每个分区先取最大的在聚合，cat 100 cat 100  初始值是100 留最大的 cat 100 一个</p>
<p>先让初始值比较出最大的，把最大的拿出来，每个分区里每个key最大的，同一个key页拿出最大</p>
<p>的。</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/cfc399390f264e9cbdcbef711b03553f/e126e273ec1045cf9c7c38f9462507e5.jpg" alt="img"></p>
<p>RDD的分区</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/4a51ec23a9e148dab3dc8a5d7e4e98ba/rdd%E7%9A%84%E5%88%86%E5%8C%BA.png" alt="img"></p>
<p>collect 执行过程</p>
<p>少量数据可以抽回driver端在放入数据库，多了数据不行，默认driver收集1G</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/385447ddc4b4403babba221f68792b4a/collect.png" alt="img"></p>
<p>分区，一个分区一个task</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/67c1a032ff8a4d878e3930f4c1412268/64b81d0bc05b408ca2385fb0c11f9203.jpg" alt="img"></p>
<p>ctrl -  </p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/7e67341d7dde40f899ed364e10492f28/5bb32075be604d52befb2d312ec9abbf.jpg" alt="img"></p>
<p>countByKey</p>
<p>countByKey 与value大小无关val rdd1 = sc.parallelize(List((“a”, 1), (“b”, 2), (“b”, 2), (“c”, 2), (“c”, 1)))rdd1.countByKeyrdd1.countByValue————————————————————————————————————————————————————————————–filterByRange对key范围过滤val rdd1 = sc.parallelize(List((“e”, 5), (“c”, 3), (“d”, 4), (“c”, 2), (“a”, 1)))包含边界b,dval rdd2 = rdd1.filterByRange(“b”, “d”)rdd2.colllect————————————————————————————————————————————————————————————–flatMapValuesval a = sc.parallelize(List((“a”, “1 2”), (“b”, “3 4”)))rdd3.flatMapValues(_.split(“ “))</p>
<p>rdd1.countByKey  按key  技术，与value没关</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/3d8a60523a8c47e9958bf96a5b03ebb2/8339e0f5ec9d44218a11d89c6e2abe35.jpg" alt="img"></p>
<p>filterByRange  按指定范围过滤</p>
<p> rdd1.filterByRange(“b”, “d”)  包含b与d</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/2fa2ce00870946bf856630c35af4b22b/db62f63809f34141abe381d32b510263.jpg" alt="img"></p>
<p>flatMapValues  拼接key与value</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/6b5e6207cf284618a0779c7ac47fe870/8faa8a0293384481a84e331bf4c0d074.jpg" alt="img"></p>
<p>拉回driver端看变成集合Array()</p>
<p>foldByKey(“”)(_+<em>),aggregateByKey(“”)(</em>+_,_+<em>),reduceByKey(</em>+_)一个函数用两次达到显示局</p>
<p>部聚合，再是全局聚合。 combineByKey？？？？？</p>
<p>四个底层都是调用的combineByKeyWithClassTag，聚合操作调用</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/06747873b08c4829927ee30fe32a13eb/60d37a0a5efa4a41bdb340cf0020c899.jpg" alt="img"></p>
<p>foreach 可能没有，取对应的task去看，取对应的目录，在executor端执行</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/582cd0cc82bc457c898062064d674ad5/dddc688617704c36ac342ae7ac935d6a.jpg" alt="img"></p>
<p>分区使用</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/86213f5f4fac481ea06fb1ab47b147ca/2617447be01c4f4fa532368c77698926.jpg" alt="img"></p>
<p>代码：</p>
<p>foreachPartitionval rdd1 = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9), 3)rdd1.foreachPartition(x =&gt; println(x.reduce(_ + _)))解释：it是每个分区是个迭代器，x是每个分区里的记录rdd1.foreachPartition(it =&gt; it.foreach(x=&gt;println(x)))</p>
<p>RDD 的五个特征</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/bf16b6a67fc440faafad372d000551e9/f60d73b1ab524efcb6829dea4c57ba28.jpg" alt="img"></p>
<p>RDD是把执行的都一起拿过来，最后执行的一块不会一个个执行。</p>
<p>RDD抽象</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/33206feca5ee4af4b6f3dedee9fe7243/rdd%E7%9A%84%E8%BF%9B%E4%B8%80%E6%AD%A5%E8%AF%B4%E6%98%8E.png" alt="img"></p>
<p>combineByKey（三个函数） 第一个函数拿数据，第二个局部，第三个全局聚合</p>
<p>先将每个值放入list中，在把局部聚合后的值放入list中，最后全局聚合合并</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/014aa13cff2140b580c84235dc531a88/combinebykey.png" alt="img"></p>
<p>combineByKey 不是很了解？？？？？？？？？？？</p>
<p>foreach（） 是一条条拿，foreachPartition() 是一个个分区拿</p>
<p>foreach，foreachPartition应用场景：例如把计算好的数据写入数据库中，但是foreach没拿一条就</p>
<p>创建一个连接，foreachPartition可以一个分区对应一个链接，任务执行是在executor,除非收集到</p>
<p>driver端。有的如count,take,top这些结果会收集到driver端，主要收集来的数据不多。</p>
<p>多台机器拉取数据需要shuffler</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/8bb268bcb33f42d7bf1f3a0d76c308c4/c2a9e99df8944165805f74600fff20a5.jpg" alt="img"></p>
<p>必须指定类型（-+-）不可以</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/8f3ec3d836f8464485a014d02b19d41c/ed2e6e7779524c96a3fb1bc9443bf214.jpg" alt="img"></p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/466e439293074211a81e3c093cb99b37/fa257fecb15e4bed816bc44ae9233966.jpg" alt="img"></p>
<p>不改变分区数量，上一个RDD多少分区下一个就多少分区，但是你改变了就变了</p>
<p>sortby() 全局排，也可以组内排（集合）</p>
<p>toLIST() 放内存中数据太大需要考虑，少量的数据可以</p>
<p>分区与分区器有区别的</p>
<p>分区器的作用：决定上游的数据到下游的哪个分区</p>
<p>上游写完了回去driver汇报，上游去拉取额时候也去driver</p>
<p>自定义分区器:</p>
<p>读来的数据最好过滤下在进行缓存</p>
<p>什么时候做checkpoint    1.迭代计算，要求保证数据安全    2.对速度要求不高（跟cache到内存进行对比）    3.将中间结果保存到hdfs  启动两个任务，一个计算一个保存防止中间结果丢失，保证迭代计算，防止从头读RDD标记checkpoint 不用记录前面的依赖关系，父RDD,因为已经保证不会丢失步骤：       设置checkpoint目录（分布式文件系统的目录hdfs目录）    经过复杂进行，得到中间结果    将中间结果checkpoint到指定的hdfs目录    后续的计算，就可以使用前面ck的数据了  最好持久化内存中防止相同的数据读两次？ 因为checkpoint会起两个任务，一个计算结果，一个放到hdfs，如果你没放到 内存中，就还会去读取，就是重新读，放到内存中不用读了直接放入hdfs就可以。 以后就把内存中释放，然后以后读hdfs中就可以了 action之前调用</p>
<p>sc.setCheckpointDir(“hdfs://s201:9000/ck2019”)  自动生成填的目录，没有回生成 。</p>
<p>代码：action才会触发</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/456c732cfd9f46898be15742ff53e782/9fe6e3b32f0940cb80aeb149530b404c.jpg" alt="img"></p>
<p>结果：做checkpoint目录</p>
<p><img src="C:/Users/%E4%B8%80%E5%88%87%E7%9C%8B%E4%BC%BC%E6%98%AF%E8%BF%90%E6%B0%94/AppData/Local/YNote/data/qq00D5989F3F6C2518635DCDFE1E5B8165/7806ddc972554c6bbef6d22b1fdf97d6/3ff8de55eae9409fb6b196dcb23a04c9.jpg" alt="img"></p>

    </div>
  </article>
  <div class="post-nav">
    <div class="post-nav-next post-nav-item">
      
        <a href="/2019/05/27/spark/" rel="next" title="Spark基础">
          <i class="fa fa-chevron-left"></i> Spark基础
        </a>
      
    </div>

    <span class="post-nav-divider"></span>

    <div class="post-nav-prev post-nav-item">
    
        <a href="/2019/07/17/spark 基础总结/" rel="prev" title="SparkRDD">
          SparkRDD <i class="fa fa-chevron-right"></i>
        </a>
      
    </div>
  </div>



      </div>
        <div class="footer">
  <div class="footer-wrapper">
    <div class="copyright">
      
      <span>&copy;</span>
      
      <span>2017 - 2019</span>
      
      <span class="author"><i class="fa fa-user"></i>迟云秋</span>
    </div>
    
      <span>由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</span>
    
    
      <span>|</span> <span>主题 - <a href="https://github.com/littleee/corazon">Corazon</a></span>
      
        <span>v1.0.0</span>
      
    
  </div>
</div>

    </main>
  </body>
</html>
